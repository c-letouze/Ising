#! /usr/bin/env python3
# PCA.py
# Coraline Letouz√©
# last revision: 30 dec 2020
# classifying the Ising phases with the first component (magnetization)

########## MODULES ##########

# array manipulation
import numpy as np
import pickle as pkl

# plot
import matplotlib.pyplot as plt
from matplotlib.cm import coolwarm, viridis, magma

# machine learning
import sklearn
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

########## FUNCTIONS ##########

def load_samples(path, size):
    """ Return a specified number of samples from 'path'. """
    with open(path, 'rb') as file_in:
        x_load, y_load = pkl.load(file_in)
    x_load, y_load = shuffle(x_load, y_load)
    x = x_load[:size]
    y = y_load[:size]
    return x, y
    
def build_train_set(size):
    """ Return a set of training samples. """
    # number of ordered + disordered samples = 130000
    # ratio ordered/disordered samples in Mehta's dataset: 7:6
    # balance the ordered/disordered samples in the training set
    size_ordered = int(size * 7/13)
    size_disordered = size - size_ordered
    #ordered set
    path_ordered = '../RawData/ordered_set.pkl'
    x_ordered, y_ordered = load_samples(path_ordered, size_ordered)
    #disordered set
    path_disordered = '../RawData/disordered_set.pkl'
    x_disordered, y_disordered = load_samples(path_disordered, size_disordered)
    # shuffle
    x = np.concatenate((x_ordered, x_disordered), axis=0)
    y = np.concatenate((y_ordered, y_disordered), axis=0)
    x, y = shuffle(x, y)
    return x, y
    
def transform_PCA(x, pca, scaler):
	return scaler.transform(pca.transform(x))
	
def sigmoid(x, w, i=0):
	return 1 / (1 + np.exp( +i +w * x))
    
########## MAIN ##########

build_datasets = False

if build_datasets:
	
	print("Building the datasets ...")
	
	# training and validation sets
	training_size = 12000
	X, Y = build_train_set(training_size)
	X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3)
	T_train, L_train = Y_train[:, 0], Y_train[:, 1]
	T_val, L_val = Y_val[:, 0], Y_val[:, 1]
	print(" - Training samples: ", np.shape(X_train))
	print(" - Training labels: ", np.shape(L_train))
	print(" - Validation samples: ", np.shape(X_val))
	
	#test set
	path_critical = '../RawData/near_critical_set.pkl'
	test_size = 5000
	X_test, Y_test = load_samples(path_critical, test_size)
	T_test, L_test = Y_test[:, 0], Y_test[:, 1]
	print(" - Test samples: ", np.shape(X_test))
	print(" - Test labels: ", np.shape(L_test))

	# save with pickle
	print("Dumping the datasets...")
	with open('PCA_datasets.pkl', 'wb') as file_out:
		dump_datasets = (X_train, X_val, X_test, 
				T_train, T_val, T_test, 
				L_train, L_val, L_test)
		pkl.dump(dump_datasets, file_out)
else:
	
	print("Loading the datasets...")
	
	with open('PCA_datasets.pkl', 'rb') as file_in:
		load_datasets = pkl.load(file_in)
		X_train, X_val, X_test,	T_train, T_val, T_test, \
			L_train, L_val, L_test = load_datasets

# make a subset for plotting
make_subset = False
if make_subset:
	print("Building a subset...")
	nb_samples_in_plot = 5000
	# ratio training:test  = 13:3
	nb_samples_from_train = int(nb_samples_in_plot * 13/16)
	nb_samples_from_test = nb_samples_in_plot - nb_samples_from_train
	print("Plotting {} samples from the training set".format(nb_samples_from_train))
	print("Plotting {} samples from the test set".format(nb_samples_from_test))
	X_plot = np.concatenate((X_train[:nb_samples_from_train], X_test[:nb_samples_from_test]), axis=0)
	T_plot = np.concatenate((T_train[:nb_samples_from_train], T_test[:nb_samples_from_test]), axis=0)
	X_plot, T_plot = shuffle(X_plot, T_plot)
	
	# save with pickle
	print("Dumping the subset...")
	with open('PCA_subset.pkl', 'wb') as file_out:
		dump_subset = (X_plot, T_plot)
		pkl.dump(dump_subset, file_out)
else:
	print("Loading the sub-dataset...")
	with open('PCA_subset.pkl', 'rb') as file_in:
		load_subset = pkl.load(file_in)
		X_plot, T_plot = load_subset
		

# Principal Component Analysis (n=2)
# print("Principal Component Analysis (n=2)...")
# pca2 = PCA(n_components=2)
# pca2.fit(X_train)
# # Results
# print("Explained variance: ", pca2.explained_variance_)
# print("Explained variance ratio: ", pca2.explained_variance_ratio_)
# print("Singular values: ", pca2.singular_values_)

# plot the component weights
plot_weights = False
if plot_weights:
	plt.imshow(pca2.components_[0].reshape(40, 40), cmap=viridis)
	plt.title('First component')
	plt.colorbar(format='%.2e', label='Weights')
	plt.savefig("../figures/pca_weights_0.png")
	plt.show()
	plt.clf()

	plt.imshow(pca2.components_[1].reshape(40, 40), cmap=magma)
	plt.title('Second component')
	plt.colorbar(format='%.2e',label='Weights')
	plt.savefig("../figures/pca_weights_1.png")
	plt.show()
	plt.clf()

gnuplot_weights = False
if gnuplot_weights:
	path0 = "../figures/pca_weights_0.dat"
	np.savetxt(path0, pca2.components_[0].reshape(40, 40), delimiter=' ')
	path1 = "../figures/pca_weights_1.dat"
	np.savetxt(path1, pca2.components_[1].reshape(40, 40), delimiter=' ')
	
# plot the data along those two components
plot_data_PCA = False
if plot_data_PCA:
	X_PCA2 = pca2.transform(X_plot)
	plt.figure(figsize=[15, 10])
	plt.scatter(X_PCA2[:, 0], X_PCA2[:, 1], c=T_plot, s=10)
	plt.colorbar(label='Temperature')
	plt.xlabel("First component")
	plt.ylabel("Second component")
	plt.title("Data samples along the two main components")
	plt.savefig("../figures/samples_PCA2.png")
	plt.show()
	plt.clf()

gnuplot_data_PCA = False
if gnuplot_data_PCA:
	X_PCA2 = pca2.transform(X_plot)
	arr = np.column_stack((X_PCA2, T_plot))
	path = '../figures/samples_PCA2.dat'
	np.savetxt(path, arr, delimiter=' ')

# Principal Component Analysis (n=1)
print("Principal Component Analysis (n=1)...")
pca = PCA(n_components=1)
X_train_PCA = pca.fit_transform(X_train)
scaler = StandardScaler()
X_train_PCA = scaler.fit_transform(X_train_PCA)
# apply
X_val_PCA = transform_PCA(X_val, pca, scaler)
X_test_PCA = transform_PCA(X_test, pca, scaler)
X_plot_PCA = transform_PCA(X_plot, pca, scaler)
# the new dataset
plot_PCA_VS_temp = False
if plot_PCA_VS_temp:
	plt.scatter(T_plot, X_plot_PCA)
	plt.title("First component w.r.t temperature")
	plt.ylabel("First component")
	plt.xlabel("Temperature")
	plt.savefig("../figures/PCA_VS_temp.png")
	plt.show()

gnuplot_PCA_VS_temp = False
if plot_PCA_VS_temp:
	arr = np.column_stack((T_plot, X_plot_PCA))
	np.savetxt("../figures/PCA_VS_temp.dat", arr, delimiter=' ')
	
# logistic regression
print("Logistic Regression...")
logReg = LogisticRegression()
logReg.fit(X_train_PCA, L_train)
score_val = logReg.score(X_val_PCA, L_val)
score_test = logReg.score(X_test_PCA, L_test)
print("Mean accuracy of Logistic Regression on: ")
print(" - Validation set: {:.4f}".format(score_val))
print(" - Test set: {:.4f}".format(score_test))
# optimization
print("Optimization...")
logReg2 = LogisticRegression()
logReg2.fit(np.abs(X_train_PCA), L_train)
score_val2 = logReg2.score(np.abs(X_val_PCA), L_val)
score_test2 = logReg2.score(np.abs(X_test_PCA), L_test)
print("Mean accuracy of Logistic Regression 2 on: ")
print(" - Validation set: {:.4f}".format(score_val2))
print(" - Test set: {:.4f}".format(score_test2))
print("coef_: ", logReg2.coef_)
print("intercept_:" ,logReg2.intercept_)
# Grid Search
# ~ print("Grid Search...")
# ~ parameters = {'C':np.logspace(-2, 2, 10)}
# ~ # default C = 1.0
# ~ print(parameters['C'])
# ~ logReg_estimator = LogisticRegression(penalty='l2', solver='lbfgs')
# ~ gs = GridSearchCV(estimator=logReg_estimator, param_grid=parameters)
# ~ gs.fit(np.abs(X_train_PCA), L_train)
# ~ print(gs.cv_results_['mean_test_score'])

# ~ scores = []
# ~ for C in parameters['C']:
	# ~ logReg = LogisticRegression(C=C)
	# ~ logReg.fit(np.abs(X_train_PCA), L_train)
	# ~ scores.append(logReg.score(np.abs(X_test_PCA), L_test))
# ~ print(scores)

mag = np.linspace(0, 1.5)
sig = sigmoid(mag, logReg2.coef_[0], logReg2.intercept_)
plot_logReg_PCA = False
if plot_logReg_PCA:
	fig, ax = plt.subplots()
	ax.scatter(np.abs(X_plot_PCA), T_plot)
	ax.set_xlabel("First component")
	ax.set_ylabel("Temperature")
	ax2 = ax.twinx()
	ax2.plot(mag, sig, 'k')
	ax2.plot([-0.05, 1.6], [0.5, 0.5], 'k:')
	ax2.set_ylabel("Probability")
	plt.title("Logistic Regression on PCA 1")
	plt.savefig("../figures/logistic_reg_PCA.png")
	plt.show()

gnuplot_logReg_PCA = True
if gnuplot_logReg_PCA:
	arr2 = np.column_stack((mag, sig))
	np.savetxt("../report/plot_logReg_PCA.dat", arr2, delimiter=' ')



